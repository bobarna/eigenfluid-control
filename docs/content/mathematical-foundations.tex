\chapter{Mathematical Foundations}
\label{sec:mathematicalFoundations}
This chapter gives a short overview of the mathematical foundations for the
techniques we discuss later on, while also establishing the notation used in
later sections.

In the following,
\begin{equation*}
    \vb{i} = \mqty( 1 \\ 0 \\ 0 )\quad
    \vb{j} = \mqty( 0 \\ 1 \\ 0 )\quad
    \vb{k} = \mqty( 0 \\ 0 \\ 1 )
\end{equation*}
will denote the canonical basis vectors.

\section{Basic Notation}

$\vb{x} \in \mathbb{R}^n$ is considered a column-matrix, i.e. $\mathbb{R}^n
= \mathbb{R}^{n \times 1}$. This also means that $\vb{x}^T$ (the transpose of
$\vb{x}$) is a row-matrix.

We denote the scalar components of a vector $\vb{x}\in \mathbb{R}^{n}$ as $(x_1,
x_2, \dots, x_n)^T$. When $\vb{x}$ denotes a position in 3D or 2D space, we also
use $\vb{x} = (x,y,z)^T$, and $\vb{x} = (x,y)^T$, respectively.

Bold uppercase letters denote matrices: $\vb{A}\in \mathbb{R}^{n \times m}$, and
its elements are indexed with $\vb{A}_{i,j}$.

A function $f(x_1, \dots, x_n)$ is a scalar-valued function $\mathbb{R}^n \to
\mathbb{R}$.  When $\vb{f}: \mathbb{R}^n \to \mathbb{R}^m$ is a vector field, we
will denote it as 

$$\vb{f}(\vb{x}) = \vb{f}(x_1,\dots, x_n) =
(f_1(\vb{x}), \dots, f_m(\vb{x}))^T$$

In keeping with the conventions of fluid mechanics literature, we use the letter
$\vb{u}$ to denote the velocity of a fluid. It is hard to say where this
notation came from, but it fits another convention to call the three components
of 3D velocity $(u, v, w)^T$ (dropping $w$ for the 2D case).

\section{Multivariable Calculus}
\subsection*{Gradient}
The gradient $\grad$ is a generalization of the derivative to multiple
dimensions. The symbol $\grad$ is called \textit{nabla}, and typically denotes
taking partial derivatives along all spatial dimensions. 

In three dimensions, 
$$\grad f(x,y,z) = \left(\pdv{f}{x}, \pdv{f}{y}, \pdv{f}{z}\right),$$ 
and in two dimensions,
$$\grad f(x,y) = \left(\pdv{f}{x}, \pdv{f}{y}\right).$$

It can be helpful to think of the gradient operator as a symbolic
vector, e.g. in three dimensions:
$$\grad = \left(\pdv{x}, \pdv{y}, \pdv{z}\right).$$ 

Taking the gradient of vector-valued functions results in a matrix of all its
first-order partial derivatives, called the \textit{Jacobian (matrix)}. With
$\vb{f}: \mathbb{R}^n \to \mathbb{R}^m$, its Jacobian takes the form:

\begin{equation}\label{eq:jacobian-matrix}
\grad \vb{f} = \vb{J}(\vb{f}) = \vb{J}_{\vb{f}}=
\mqty[\grad{f_1}\\\grad{f_2}\\\vdots\\\grad f_n]^T = 
\mqty[
\pdv{f_1}{x_1} & \pdv{f_1}{x_2} & \dots & \pdv{f_1}{x_n}\\
\pdv{f_2}{x_1} & \pdv{f_2}{x_2} & \dots & \pdv{f_2}{x_n}\\
\vdots         &    \vdots      & \ddots & \vdots \\
\pdv{f_m}{x_1} & \pdv{f_m}{x_2} & \dots & \pdv{f_m}{x_n}
].
\end{equation}

When $\vb{f}(x_1,\dots x_n) = (f_1, \dots, f_n)^T$, i.e. $\vb{f}: \mathbb{R}^n
\to \mathbb{R}^n$, mapping the $n$ dimensional Euclidean space onto itself,
its determinant is called the \textit{Jacobian determinant}.

\subsection*{Divergence}
The divergence operator measures how much the values of a vector field are
converging or diverging at any point in space. In three dimensions:

$$\div{\vb{u}(x,y,z)} = \div(u(\vb{x}), v(\vb{x}), w(\vb{x}))^T = 
\pdv{u}{x}+\pdv{v}{y}+\pdv{w}{z}.$$

Note that the input is a vector, and the output is a scalar, i.e. $\vb{u}:
\mathbb{R}^3 \to \mathbb{R}^3, \div{\vb{u}}: \mathbb{R}^3\to\mathbb{R}$.
Heuristically, in the case of a fluid velocity field $\vb{u}$, this translates
to a measure of whether a given point acts as a source, or a sink, i.e. whether
particles are created or lost in that infinitesimal region. (Later on, we will
come back to the notion of a divergence-free fluid.)

\subsection*{Curl}\label{section:curl}
The curl operator measures how much a vector field is rotating around any point. 
In three dimensions, it is given by the vector
$$\curl{\vb{u}(x,y,z) = 
    \mqty(\pdv*{x} \\ \pdv*{y} \\  \pdv*{z})^T \cross 
    \mqty(u(x,y,z) \\ v(x,y,z) \\ w(x,y,z))
= \mqty|
    \vb{i}   & \vb{j}   & \vb{k}   \\
    \pdv*{x} & \pdv*{y} & \pdv*{z} \\
    u        & v        & w
|} = \mqty(
\pdv*{w}{y} - \pdv*{v}{z} \\
\pdv*{u}{z} - \pdv*{w}{x} \\
\pdv*{v}{x} - \pdv*{u}{y}
).$$

We can reduce this formula to two dimensions by taking the third component of
the expression above, as if we were looking at the three-dimensional vector
field $(u,v,0)$. Thus, the two-dimensional curl is a scalar:
$$\curl{\vb{u}(x,y)} = 
    \mqty(\pdv*{x} \\ \pdv*{y}) \cross 
    \mqty(u(x,y) \\ v(x,y))
    = \pdv{v}{x} - \pdv{u}{y}.$$

We can also interpret this value as the third component of a three-dimensional
vector, perpendicular to the vector field $(u,v,0)$:
$$\curl{\vb{u}(x,y)} = 
    \mqty(\pdv*{x} \\ \pdv*{y}) \cross 
    \mqty(u(x,y) \\ v(x,y))
    = 
    \mqty(0\\0\\\pdv{v}{x} - \pdv{u}{y}).$$


\subsection*{Material Derivative}
For a velocity $\vb{u}(t,x,y,z) = \mqty(u \\ v \\ w)$, 
we define the material derivative as 
$$\dv{\vb{u}}{t} = \pdv{\vb{u}}{t} + \qty(\vb{u}\vdot\grad)\vb{u} ,$$

a special case of the total derivative. As $x, y, z$ describe the spatial
position of a particle traveling through space over time, they depend on time
$t$ themselves, i.e. $\vb{u}(t, x(t), y(t), z(t))$. Utilizing the chain rule,
we can arrive on the above definition by taking the total derivative of
$\vb{u}(t, x(t), y(t), z(t))$, and rearranging the terms:

\begin{alignat*}{2}\label{eq:material-derivative}
    \dv{\vb{u}}{t} &= \pdv{\vb{u}}{t}\dv{t}{t} 
                    + \pdv{\vb{u}}{x}\dv{x}{t} 
                    + \pdv{\vb{u}}{y}\dv{y}{t} 
                    + \pdv{\vb{u}}{z}\dv{z}{t} \\
                    &= \pdv{\vb{u}}{t} 1 \quad
                    + \pdv{\vb{u}}{x} u \quad
                    + \pdv{\vb{u}}{y} v \quad
                    + \pdv{\vb{u}}{z} w \\
                    &= \pdv{\vb{u}}{t} 1 \quad
                    + u \pdv{\vb{u}}{x} \quad
                    + v \pdv{\vb{u}}{y} \quad
                    + w \pdv{\vb{u}}{z} \\
                    &= \pdv{\vb{u}}{t} +
                    \qty(
                        \vb{u}
                        \vdot
                        \left[ \pdv{x}, \pdv{y}, \pdv{z} \right]
                    ) \vdot \vb{u} \\
                    &= \pdv{\vb{u}}{t}
                    + \qty(\vb{u}\vdot\grad)\vdot\vb{u}.
\end{alignat*}

\subsection*{Laplacian}
The Laplacian operator is defined as the divergence of the gradient of a scalar
function $f$. In general, for $f(\vb{x}): \mathbb{R}^n \to \mathbb{R}$, it is
given by

    $$\Delta f = \grad^2{f} = \grad \vdot \grad f = \sum_{i=1}^n
\pdv[2]{f}{x_i}.$$

In three dimensions, this reduces to 
$$\grad \vdot \grad f = \pdv[2]{f}{x} + \pdv[2]{f}{y} + \pdv[2]{f}{z},$$

and in two dimensions,

$$ \grad \vdot \grad f = \pdv[2]{f}{x} + \pdv[2]{f}{y}.$$

Taking the divergence of the gradient corresponds to an averaging of how much
a value at a given position differs from its neighborhood. As an example, we
can look at a non-static scalar field, with a rate of change proportional to
its Laplacian with proportionality $\alpha$:

$$
    \pdv{\Phi}{t} = \alpha \grad^2\Phi,
$$

which is the well-known heat equation, and governs diffusion of the field. It is
essentially a smoothing operation that results in an averaging of values at
every point in space. Using heat as an analogy, hot and cold spots diffuse
throughout the field, resulting in a more uniform distribution of the heat in
the field, eventually reaching a uniform distribution of heat.

\subsection*{Vector Laplacian}
\label{section:vector-laplacian}
The Laplacian can also be applied to vector (or even matrix) fields, and the
result is simply the Laplacian of each component.

Essentially, the vector Laplacian is what we have been building towards so far,
as this operator is going to be the cornerstone of the eigenfluids simulation
technique in section~\ref{section:laplacian-eigenfluids}. As such, we will show
some important properties of this operator, and will return to these in later
sections.

The vector Laplacian of a vector field $\vb{f}$ is defined as

$$\underbrace{\Delta \vb{f}}_{\text{vector Laplacian}}
= \underbrace{\grad(\div{\vb{f}})}_{\text{gradient of the divergence}}
- \underbrace{\curl(\curl{\vb{f}})}_{\text{curl of curl} = \text{curl}^2}
= \text{grad}(\text{div}(f))
- \underbrace{\text{curl}(\text{curl}(f))}_{\text{curl}^2(f)}$$

In Cartesian coordinates, the vector Laplacian simplifies to taking the
Laplacian of each component:

\begin{equation}
    \Delta\vb{f}(x, y, z) = (\grad\vdot\grad)\vb{f} = 
    \mqty[ \Delta f_1 \\ \Delta f_2 \\ \Delta f_3 ] =
    \mqty[
        \pdv[2]{f_1}{x} + \pdv[2]{f_1}{y} + \pdv[2]{f_1}{z}\\
        \pdv[2]{f_2}{x} + \pdv[2]{f_2}{y} + \pdv[2]{f_2}{z}\\
        \pdv[2]{f_3}{x} + \pdv[2]{f_3}{y} + \pdv[2]{f_3}{z}
    ].
    \label{eq:laplacian}
\end{equation}

We can see that these are equivalent by writing out
$\text{grad}(\text{div}(\vb{f})) - \text{curl}(\vb{f})$ explicitly:

\begin{equation}\label{eq:graddiv-identity}
    \grad(\div{\vb{f}}) - \curl(\curl{\vb{f}}) =
    \mqty[
        \pdv[2]{f_1}{x} + 
        \cancel{\pdv[2]{f_2}{x}{y}} + 
        \cancel{\pdv[2]{f_3}{x}{z}}
        \\
        \cancel{\pdv[2]{f_1}{y}{x}} + 
        \pdv[2]{f_2}{y} + 
        \cancel{\pdv[2]{f_3}{y}{z}}
        \\
        \cancel{\pdv[2]{f_1}{z}{x}} +
        \cancel{\pdv[2]{f_2}{z}{y}} + 
        \pdv[2]{f_3}{z}
    ] -
    \mqty[
        \cancel{\pdv[2]{f_2}{x}{y}} - 
        \pdv[2]{f_1}{y} - 
        (\pdv[2]{f_1}{z} - \cancel{\pdv[2]{f_3}{x}{z}})
        \\
        \cancel{\pdv[2]{f_3}{y}{z}} - 
        \pdv[2]{f_2}{z} - 
        ({\pdv[2]{f_2}{x}} - \cancel{\pdv[2]{f_1}{y}{x}})
        \\
        \cancel{\pdv[2]{f_1}{z}{x}} - 
        \pdv[2]{f_3}{x} - 
        (\pdv[2]{f_3}{y} - \cancel{\pdv[2]{f_2}{z}{y}})
    ],
\end{equation}

where the mixed second order partial derivatives cancel each other out, giving
us equation~\eqref{eq:laplacian}.


\subsection*{Differential Identities}

It can be shown \cite{vector-analysis1901} that for any smooth function
$\vb{u}$, 
\begin{align*}
    \numberthis
    \label{eq:diff-identity}
    \div(\curl{\vb{u}}) &\equiv 0, \\
    \curl(\grad{\vb{u}}) &\equiv 0.
\end{align*}

The idea behind the Helmholtz or Hodge decomposition is that any vector field
$\vb{u}$ can be written as the composition of a divergence-free part, and
a curl-free part. Making use of Equations~\eqref{eq:diff-identity}, we can write
the divergence-free part as the curl of something, and the curl-free part can be
written as the gradient of something else. In three dimensions,
$$\vb{u} = \curl{\boldsymbol\Psi} - \grad{p},$$

where $\boldsymbol\Psi$ is a vector-valued potential function, and $p$ is
a scalar potential function. In two dimensions, $\Psi$ is also scalar:

$$\vb{u} = \curl{\Psi} - \grad{p}.$$

This decomposition technique becomes highly relevant for incompressible fluid
flows, where we would like to make our velocity field $\vb{u}$ divergence-free
(i.e. no particles should be lost or created). Simulation techniques often
decompose an intermediate fluid field $\vb{u}^{t+1}$ into a divergence-free
part, and interpreting $p$ as the pressure that is keeping the fluid flow
divergence-free, usually throwing away the values of $p$ immediately.

Rearranging equation~\eqref{eq:graddiv-identity}, we can derive another useful
identity:
$$\curl(\curl{\vb{u}}) \equiv \grad(\div{\vb{u}}) - \div{\grad{\vb{u}}}.$$

\section{Optimization}\label{section:optimization}
Iterative optimization algorithms look for a solution by iteratively applying
some update step $\Delta$ to some starting parameter $\vb{x}_0$, giving an
estimation of how to approach some optimal parameter $\vb{x}^*$, with the goal
to continuously lower the error as defined by a loss function $L$.  We address
optimization scenarios where the target is to minimize a scalar-valued loss
function $L(\vb{x}): \mathbb{R}^N \to \mathbb{R}$  with respect to one of its
inputs:

$$\label{eq:optimization-target}
\arg\min_{\vb{x}} L(\vb{x}) = \vb{x}^*.$$

Among the vast number of established optimization algorithms, \acf{GD} is the
most basic and straightforward. Making use of the Jacobian matrix as defined in
\eqref{eq:jacobian-matrix}, it gives us an update step $\Delta$ given
a parameter $\vb{x}$, consisting of the transposed Jacobian matrix of $\vb{f}$
scaled by a scalar \textit{learning rate} $\lambda$. Repeatedly applying the
update step $\Delta(\vb{x}) = -\lambda J^T(\vb{x})$, the steps of a \acf{GD}
optimization can be written out as:

\begin{align*}\label{eq:gd-steps}\numberthis
    \vb{x}_0  & \\
    \vb{x}_{1} &= \vb{x}_0 - \lambda\Delta = \vb{x}_0 - \lambda
        J_{L}^T(\vb{x}_0) = \vb{x}_0 - \lambda \grad{L}^T(\vb{x}_0) \\
    \vdots&\\
    \vb{x}_t &= \vb{x}_{t-1} - \lambda J_L^T(\vb{x}_{t-1})\\
    \vb{x}^* &= \vb{x}_t - \lambda J_L^T(\vb{x}_t),
\end{align*}

which is exactly what we will be utilizing in our first couple of optimization
scenarios in chapter~\ref{chapter:controlling-laplacian-eigenfluids}. Note that
as our loss is scalar-valued, the transposed Jacobian matrix of $L$ has the same
dimensionality as the input $\vb{x}$, i.e. $J_{L}^T \in \mathbb{R}^{N\times 1};$
$\vb{x} \in \mathbb{R}^{N\times 1}$, making them both a column vector of size
$N$, which means that they can indeed be added together. 

We can also think about these optimization steps as continuously moving towards
some locally observed lowest point in the error landscape. The gradient
$\grad{L}$ is giving us the direction of steepest \textit{ascent}, which means
that the opposite direction, $-\grad{L}$ will be the direction of the steepest
\textit{descent} locally, guiding us towards some (potentially only local) error
minimum.

We show examples of utilizing the \ac{GD} steps as written in
\eqref{eq:gd-steps} for minimizing the error between a simulated and target
state of physical simulations in
sections~\ref{section:matching-velocities},\ref{section:initial-velocity}, and
\ref{section:cfe}.

\subsection{Neural Networks}\label{section:neural-networks}
The goal of \acf{DL} is to approximate an unknown function 

$$\vb{f}^*(\vb{x}) = \vb{y}^*,$$

where $\vb{y}^*$ denotes \textit{ground truth} solutions. $\vb{f}^*(\vb{x})$ is
approximated by a \acf{NN} representation 

$$\vb{f}(\vb{x}, \boldsymbol\theta) = \vb{y},$$

where $\boldsymbol{\theta}$ is a vector of \textit{weights}, influencing the output of
the \ac{NN}. \ac{DL} is about finding $\boldsymbol{\theta}$ parameters such
that the outputs of the \ac{NN} match the $\vb{y}^*$ outputs of the original
function $\vb{f}^*$ as closely as possible, as measured by some scalar-valued
loss function $L$:

$$\arg\min_{\boldsymbol\theta} L\big(\vb{f}(\vb{x}, \boldsymbol\theta), \vb{y}^*
    \big).$$

In the simplest case, using a mean-square error (also known as $L_2$
norm):

\begin{equation}\label{eq:NN-min}
    \arg\min_{\boldsymbol\theta} 
        L_2\big(\vb{f}(\vb{x}, \boldsymbol\theta), \vb{y}^* \big) 
    = \arg\min_{\boldsymbol\theta} 
        \lvert \vb{f}(\vb{x}, \boldsymbol\theta) - \vb{y}^* \rvert_2^2.
\end{equation}

As discussed in section~\ref{section:optimization}, we can use the
gradients of the loss function $L$ with respect to the weights
$\boldsymbol{\theta}$ (i.e. $\pdv*{L}{\boldsymbol\theta}$) to solve
equation~\eqref{eq:NN-min}, yielding the optimal $\boldsymbol{\theta}$
parameters. We optimize, i.e. \textit{train} our \ac{NN} with a \acf{SGD}
optimizer, such as Adam \cite{adam}.

In the case of a fully-connected \ac{NN}, we can write its $i^{th}$ layer as 

\begin{equation}\label{nn-single-layer-math}
    \vb{o}^i = \sigma\left(\vb{W}_i \vb{o}^{i-1} + b_i\right),
\end{equation}

where $\vb{o}^i$ is the output of the $i^{th}$ layer, $\sigma$ is a non-linear
activation function, such as the rectified linear unit (ReLU) function, and
$\vb{W}_i$ and $b_i$ are the weight matrix and the bias of layer $i$,
respectively. We call $\vb{W}_i$ and $b_i$ the parameters of the \ac{NN},
and collect their values from all layers in $\boldsymbol\theta$.

It is worth explicitly noting that the term \acf{DL} is referring to a "deep"
\acf{NN}, meaning that many layers are stringed after each other. People usually
refer to a \ac{NN} as \ac{DL}, when its architecture consists of more than three
layers. In turn, both \acp{NN} and \ac{DL} are a subset of the more general
paradigm of \acf{ML}. Finally, \acf{AI} is the broadest term used to classify
any and all techniques that aim to create a form of (human-like) intelligence in
computers.

In the context of \ac{DL}, it is helpful to think of the derivative as
\textit{function sensitivity}, denoting how a small change in an input variable
changes the output of the function.  For finding the $\boldsymbol\theta$
parameters of a \ac{NN}, this is exactly what we need: how to tweak
$\boldsymbol{\theta}$ to reduce the output of a loss function
$L(\vb{x},\vb{\theta})$.

As we already showed in equation~\eqref{eq:material-derivative}, the chain rule
gives us the derivatives of composite functions. For a multivariable function
$f: \mathbb{R}^2 \to \mathbb{R}$, it can be summarized as:
\begin{align}
    \dv{}{t}f(x(t), y(t)) &=  \pdv{f}{x}\pdv{x}{t} + \pdv{f}{y}\pdv{y}{t}\\
    \intertext{and expressed with vector notation}
    =\pdv{}{t}f \circ \vb{x}(t) &= \pdv{}{t}f(\vb{x}(t)) = \grad{f} \cdot \vb{x}(t)
\end{align}

As we will use it in section~\ref{section:nn-training}, a fully-connected
\ac{NN} can be written as a function composition of layers and activation
functions $\sigma$. In the case of a simple fully-connected \ac{NN} with layer
weights $\vb{W}_i$ and $\text{ReLU}(\vb{x})= \text{max}(0, \vb{x})$ activation
functions, this becomes:

\begin{align*}\label{eq:nn-layers-math}\numberthis
    \qq*{linear function:} 
    f(\vb{x}, \boldsymbol{\theta}) &= \vb{W}\vb{x} 
    \\
    \qq*{2 layers:} 
    f(\vb{x}, \boldsymbol{\theta}) &= 
    \vb{W}_2\text{max}(\vb{0}, \vb{W}_1\vb{x})
    \\
    \qq*{3 layers:} 
    f(\vb{x}, \boldsymbol{\theta}) &=
    \vb{W}_3\text{max}(\vb{0}, \vb{W}_2\text{max}(\vb{0}, \vb{W}_1\vb{x})) 
    \\
    \qq*{4 layers:} 
    f(\vb{x}, \boldsymbol{\theta}) &= \vb{W}_4 \text{max}(\vb{0},
    \vb{W}_3\text{max}(\vb{W}_2\text{max}(\vb{0}, \vb{W}_1\vb{x})))
    \\
   \dots
\end{align*}

Note that, as in equation~\eqref{nn-single-layer-math}, there is an additional
$+b_i$ scalar addition at each layer that we did not write out in
\eqref{eq:nn-layers-math} for clarity.

Given some target values $\vb{y}^*$, also known as \textit{ground truth data},
we can use a scalar (also known as \textit{regression}) loss to measure the
error of the predictions of the \ac{NN}:
\begin{align}
    \qq*{L1 loss} &L(\vb{x},\vb{y}^*, \boldsymbol\theta) =
        \frac{1}{N} \sum_i^N 
        \norm{\vb{f}(\vb{x}, \boldsymbol\theta) - \vb{y}^*}_1
    \\\label{eq:l2}
    \qq*{MSE (or $L_2^2$) loss} &L(\vb{x}, \vb{y}^*, \boldsymbol\theta) =
        \frac{1}{N} \sum_i^N 
        \norm{\vb{f}(\vb{x}, \boldsymbol\theta) - \vb{y}^*}_2^2,
\end{align}

where the \acf{MSE} is the squared $L_2$ loss. The $L_2$ norm of a vector is its
length in Euclidean space. In 3D: $L_2(\vb{x}) = \sqrt{x^2 + y^2 + z^2}$, and we
will also denote it later on as $\abs{\cdot}_2^2$.

In the context of \acf{PBDL}, it is especially important to understand the flow
of gradients, as in the next chapter, we will integrate differentiable physical
simulations into this learning setup.

Regarding nomenclature, in classical literature, \textit{adjoint method} and
\textit{reverse mode differentiation} are equivalent names for backpropagation.





