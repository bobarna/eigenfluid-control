\chapter{Mathematical Foundations}
\label{sec:mathematicalFoundations}
This chapter gives a short overview of the mathematical foundations for the
techniques we discuss later on, while also establishing the notation used in
later sections.

In the following,
\begin{equation*}
    \vb{i} = \mqty( 1 \\ 0 \\ 0 )\quad
    \vb{j} = \mqty( 0 \\ 1 \\ 0 )\quad
    \vb{k} = \mqty( 0 \\ 0 \\ 1 )
\end{equation*}
will denote the canonical basis vectors.

\section{Basic Notation}

$\vb{x} \in \mathbb{R}^n$ is considered a column-matrix, i.e. $\mathbb{R}^n
= \mathbb{R}^{n \times 1}$. This also means that $\vb{x}^T$ (the transpose of
$\vb{x}$) is a row-matrix.

We denote the scalar components of a vector $\vb{x}\in \mathbb{R}^{n}$ as $(x_1,
x_2, \dots, x_n)^T$. When $\vb{x}$ denotes a position in 3D or 2D space, we also
use $\vb{x} = (x,y,z)^T$, and $\vb{x} = (x,y)^T$, respectively.

Bold uppercase letters denote matrices: $\vb{A}\in \mathbb{R}^{n \times m}$, and
its elements are indexed with $\vb{A}_{i,j}$

A function $f(x_1, \dots, x_n)$ is a scalar-valued function $\mathbb{R}^n \to
\mathbb{R}$.  When $\vb{f}: \mathbb{R}^n \to \mathbb{R}^m$ is a vector field, we
will denote it as 

$$\vb{f}(\vb{x}) = \vb{f}(x_1,\dots, x_n) =
(f_1(\vb{x}), \dots, f_m(\vb{x}))^T$$

In keeping with the conventions of fluid mechanics literature, we use the letter
$\vb{u}$ to denote the velocity of a fluid. It is hard to say where this
notation came from, but it fits another convention to call the three components
of 3D velocity $(u, v, w)^T$ (dropping $w$ for the 2D case).

\section{Multivariable Calculus}
\subsection*{Gradient}
The gradient $\grad$ is a generalization of the derivative to multiple
dimensions. The symbol $\grad$ is called \textit{nabla}, and typically denotes
taking partial derivatives along all spatial dimensions. 

In three dimensions, 
$$\grad f(x,y,z) = \left(\pdv{f}{x}, \pdv{f}{y}, \pdv{f}{z}\right),$$ 
and in two dimensions,
$$\grad f(x,y) = \left(\pdv{f}{x}, \pdv{f}{y}\right).$$

It can be helpful to think of the gradient operator as a symbolic
vector, e.g. in three dimensions:
$$\grad = \left(\pdv{x}, \pdv{y}, \pdv{z}\right).$$ 

Taking the gradient of vector-valued functions results in a matrix of all its
first-order partial derivates, called the \textit{Jacobian (matrix)}. With
$\vb{f}: \mathbb{R}^n \to \mathbb{R}^m$, its Jacobian takes the form:

\begin{equation}\label{eq:jacobian-matrix}
\grad \vb{f} = \vb{J}(\vb{f}) = \vb{J}_{\vb{f}}=
\mqty[\grad{f_1}\\\grad{f_2}\\\vdots\\\grad f_n]^T = 
\mqty[
\pdv{f_1}{x_1} & \pdv{f_1}{x_2} & \dots & \pdv{f_1}{x_n}\\
\pdv{f_2}{x_1} & \pdv{f_2}{x_2} & \dots & \pdv{f_2}{x_n}\\
\vdots         &    \vdots      & \ddots & \vdots \\
\pdv{f_m}{x_1} & \pdv{f_m}{x_2} & \dots & \pdv{f_m}{x_n}
].
\end{equation}

When $\vb{f}(x_1,\dots x_n) = (f_1, \dots, f_n)^T$, i.e. $\vb{f}: \mathbb{R}^n
\to \mathbb{R}^n$, mapping the $n$ dimensional Euclidean space onto itself,
its determinant is called the \textit{Jacobian determinant}.

\subsection*{Divergence}
The divergence operator measures how much the values of a vector field are
converging or diverging at any point in space. In three dimensions:

$$\div{\vb{u}(x,y,z)} = \div(u(\vb{x}), v(\vb{x}), w(\vb{x}))^T = 
\pdv{u}{x}+\pdv{v}{y}+\pdv{w}{z}.$$

Note that the input is a vector, and the output is a scalar, i.e. $\vb{u}:
\mathbb{R}^3 \to \mathbb{R}^3, \div{\vb{u}}: \mathbb{R}^3\to\mathbb{R}$.
Heuristically, in the case of a fluid velocity field $\vb{u}$, this translates
to a measure of whether a given point acts as a source, or a sink, i.e. whether
particles are created or lost in that infinitesimal region. (Later on, we will
come back to the notion of a divergence-free fluid.)

\subsection*{Curl}
The curl operator measures how much a vector field is rotating around any point. 
In three dimensions, it is given by the vector
$$\curl{\vb{u}(x,y,z) = 
    \mqty(\pdv*{x} \\ \pdv*{y} \\  \pdv*{z})^T \cross 
    \mqty(u(x,y,z) \\ v(x,y,z) \\ w(x,y,z))
= \mqty|
    \vb{i}   & \vb{j}   & \vb{k}   \\
    \pdv*{x} & \pdv*{y} & \pdv*{z} \\
    u        & v        & w
|} = \mqty(
\pdv*{w}{y} - \pdv*{v}{z} \\
\pdv*{u}{z} - \pdv*{w}{x} \\
\pdv*{v}{x} - \pdv*{u}{y}
).$$

We reduce this formula to two dimensions by taking the third component of
the expression above, as if we were looking at the three-dimensional vector
field $(u,v,0)$. Thus, the two-dimensional curl is a scalar:
$$\curl{\vb{u}(x,y)} = 
    \mqty(\pdv*{x} \\ \pdv*{y}) \cross 
    \mqty(u(x,y) \\ v(x,y))
    = \pdv{v}{x} - \pdv{u}{y}.$$

\subsection*{Material Derivative}
For a velocity $\vb{u}(t,x,y,z) = \mqty(u \\ v \\ w)$, 
we define the material derivative as 
$$\dv{\vb{u}}{t} = \pdv{\vb{u}}{t} + \qty(\vb{u}\vdot\grad)\vb{u} ,$$

a special case of the total derivative. As $x, y, z$ describe the spatial
position of a particle traveling through space over time, they depend on time
$t$ themselves, i.e. $\vb{u}(t, x(t), y(t), z(t))$. Utilizing the chain rule,
we can arrive on the above definition by taking the total derivative of
$\vb{u}(t, x(t), y(t), z(t))$, and rearranging the terms:

\begin{alignat*}{2}
    \dv{\vb{u}}{t} &= \pdv{\vb{u}}{t}\dv{t}{t} 
                    + \pdv{\vb{u}}{x}\dv{x}{t} 
                    + \pdv{\vb{u}}{y}\dv{y}{t} 
                    + \pdv{\vb{u}}{z}\dv{z}{t} \\
                    &= \pdv{\vb{u}}{t} 1 \quad
                    + \pdv{\vb{u}}{x} u \quad
                    + \pdv{\vb{u}}{y} v \quad
                    + \pdv{\vb{u}}{z} w \\
                    &= \pdv{\vb{u}}{t} 1 \quad
                    + u \pdv{\vb{u}}{x} \quad
                    + v \pdv{\vb{u}}{y} \quad
                    + w \pdv{\vb{u}}{z} \\
                    &= \pdv{\vb{u}}{t} +
                    \qty(
                        \vb{u}
                        \vdot
                        \left[ \pdv{x}, \pdv{y}, \pdv{z} \right]
                    ) \vdot \vb{u} \\
                    &= \pdv{\vb{u}}{t}
                    + \qty(\vb{u}\vdot\grad)\vdot\vb{u}.
\end{alignat*}

\subsection*{Laplacian}
The Laplacian operator defined as the divergence of the gradient of a scalar
function $f$. In general, for $f(\vb{x}): \mathbb{R}^n \to \mathbb{R}$, it is
given by

    $$\Delta f = \grad^2{f} = \grad \vdot \grad f = \sum_{i=1}^n
\pdv[2]{f}{x_i}.$$

In three dimensions, this reduces to 
$$\grad \vdot \grad f = \pdv[2]{f}{x} + \pdv[2]{f}{y} + \pdv[2]{f}{z},$$

and in two dimensions,

$$ \grad \vdot \grad f = \pdv[2]{f}{x} + \pdv[2]{f}{y}.$$

Taking the divergence of the gradient corresponds to an averaging of how much
a value at a given position differs from its neighborhood. As an example, we
can look at a non-static scalar field, with a rate of change proportional to
its Laplacian with proportionality $\alpha$:

$$
    \pdv{\Phi}{t} = \alpha \div^2\Phi,
$$

which is the well-known heat equation, and governs diffusion of the field. It is
essentially a smoothing operation that results in an averaging of values at
every point in space. Using heat as an analogy, hot and cold spots diffuse
throughout the field, resulting in a more uniform distribution of the heat in
the field, eventually reaching a uniform distribution of heat.

\subsection*{Vector Laplacian}
\label{section:vector-laplacian}
The Laplacian can also be applied to vector (or even matrix) fields, and the
result is simply the Laplacian of each component.

Essentially, the vector Laplacian is what we have been building towards so far,
as this operator is going to be the cornerstone of the eigenfluids simulation
technique.\todo{link to section} As such, we will show some important properties
of this operator. We will return to these in later sections.\todo{link to
section}

The vector Laplacian of a vector field $\vb{f}$ is defined as

$$\underbrace{\Delta \vb{f}}_{\text{vector Laplacian}}
= \underbrace{\grad(\div{\vb{f}})}_{\text{gradient of the divergence}}
- \underbrace{\curl(\curl{\vb{f}})}_{\text{curl of curl} = \text{curl}^2}
= \text{grad}(\text{div}(f))
- \underbrace{\text{curl}(\text{curl}(f))}_{\text{curl}^2(f)}$$

In Cartesian coordinates, the vector Laplacian simplifies to taking the
Laplacian of each component:

\begin{equation}
    \Delta\vb{f}(x, y, z) = (\grad\vdot\grad)\vb{f} = 
    \mqty[ \Delta f_1 \\ \Delta f_2 \\ \Delta f_3 ] =
    \mqty[
        \pdv[2]{f_1}{x} + \pdv[2]{f_1}{y} + \pdv[2]{f_1}{z}\\
        \pdv[2]{f_2}{x} + \pdv[2]{f_2}{y} + \pdv[2]{f_2}{z}\\
        \pdv[2]{f_3}{x} + \pdv[2]{f_3}{y} + \pdv[2]{f_3}{z}
    ].
    \label{eq:laplacian}
\end{equation}

We can see that these are equivalent by writing out
$\text{grad}(\text{div}(\vb{f})) - \text{curl}(\vb{f})$ explicitly:

\begin{equation}\label{eq:graddiv-identity}
    \grad(\div{\vb{f}}) - \curl(\curl{\vb{f}}) =
    \mqty[
        \pdv[2]{f_1}{x} + 
        \cancel{\pdv[2]{f_2}{x}{y}} + 
        \cancel{\pdv[2]{f_3}{x}{z}}
        \\
        \cancel{\pdv[2]{f_1}{y}{x}} + 
        \pdv[2]{f_2}{y} + 
        \cancel{\pdv[2]{f_3}{y}{z}}
        \\
        \cancel{\pdv[2]{f_1}{z}{x}} +
        \cancel{\pdv[2]{f_2}{z}{y}} + 
        \pdv[2]{f_3}{z}
    ] -
    \mqty[
        \cancel{\pdv[2]{f_2}{x}{y}} - 
        \pdv[2]{f_1}{y} - 
        (\pdv[2]{f_1}{z} - \cancel{\pdv[2]{f_3}{x}{z}})
        \\
        \cancel{\pdv[2]{f_3}{y}{z}} - 
        \pdv[2]{f_2}{z} - 
        ({\pdv[2]{f_2}{x}} - \cancel{\pdv[2]{f_1}{y}{x}})
        \\
        \cancel{\pdv[2]{f_1}{z}{x}} - 
        \pdv[2]{f_3}{x} - 
        (\pdv[2]{f_3}{y} - \cancel{\pdv[2]{f_2}{z}{y}})
    ],
\end{equation}

where the mixed second order partial derivatives cancel each other out, giving
us equation~\eqref{eq:laplacian}.


\subsection*{Differential Identities}

It can be shown that for any smooth function $\vb{u}$, 
\begin{align*}
    \numberthis
    \label{eq:diff-identity}
    \div(\curl{\vb{u}}) &\equiv 0, \\
    \curl(\grad{\vb{u}}) &\equiv 0.
\end{align*}

The idea behind the Helmholtz or Hodge decomposition is that any vector field
$\vb{u}$ can be written as the composition of a divergence-free part, and
a curl-free part. Making use of Equations~\eqref{eq:diff-identity}, we can write
the divergence-free part as the curl of something, and the curl-free part can be
written as the gradient of something else. In three dimensions,
$$\vb{u} = \curl{\boldsymbol\Psi} - \grad{p},$$

where $\boldsymbol\Psi$ is a vector-valued potential function, and $p$ is
a scalar potential function. In two dimensions, $\Psi$ is also scalar:

$$\vb{u} = \curl{\Psi} - \grad{p}.$$

This decomposition technique becomes highly relevant for incompressible fluid
flows, where we would like to make our velocity field $\vb{u}$ divergence-free
(i.e. no particles should be lost or created). Simulation techniques often
decompose an intermediate fluid field $\vb{u}^{t+1}$ into a divergence-free
part, and interpreting $p$ as the pressure that is keeping the fluid flow
divergence-free, usually throwing away the values of $p$ immediately.

Rearranging equation~\eqref{eq:graddiv-identity}, we can derive another useful
identity that will come up later in section~(\todo{ref}):
$$\curl(\curl{\vb{u}}) \equiv \grad(\div{\vb{u}}) - \div{\grad{\vb{u}}}.$$

\section{Optimization}\label{section:optimization}
Iterative optimization algorithms look for a solution by iteratively applying
some update step $\Delta$ to some starting parameter $\vb{x}_0$, giving an
estimation of how to approach some optimal parameter $\vb{x}^*$, with the goal
to continuously lower the error as defined by a loss function $L$.  We address
optimization scenarios where the target is to minimize a scalar-valued loss
function $L(\vb{x}): \mathbb{R}^N \to \mathbb{R}$  with respect to one of its
inputs:

$$\label{eq:optimization-target}
\arg\min_{\vb{x}} L(\vb{x}) = \vb{x}^*.$$

Among the vast number of established optimization algorithms, \acf{GD} is the
most basic and straightforward. Making use of the Jacobian matrix as defined in
\eqref{eq:jacobian-matrix}, it gives us an update step $\Delta$ given
a parameter $\vb{x}$, consisting of the transposed Jacobian matrix of $\vb{f}$
scaled by a scalar \textit{learning rate} $\lambda$. Repeatedly applying the
update step $\Delta(\vb{x}) = -\lambda J^T(\vb{x})$, the steps of a \acf{GD}
optimization can be written out as:

\begin{align*}\label{eq:gd-steps}\numberthis
    \vb{x}_0  & \\
    \vb{x}_{1} &= \vb{x}_0 - \lambda\Delta \equiv \vb{x}_0 - \lambda
        J_{L}^T(\vb{x}_0) \equiv \vb{x}_0 - \lambda \grad{L}^T(\vb{x}_0) \\
    \vdots&\\
    \vb{x}_t &= \vb{x}_{t-1} - \lambda J_L^T(\vb{x}_{t-1})\\
    \vb{x}^* &= \vb{x}_t - \lambda J_L^T(\vb{x}_t),
\end{align*}

which is exactly what we will be utilizing in our first couple of optimization
scenarios in chapter~\todo{Reference chapter}. Note that as our loss is
scalar-valued, the transposed Jacobian matrix of $L$ has the same dimensionality
as the input $\vb{x}$, i.e. $J_{L}^T \in \mathbb{R}^{N\times 1};$ $\vb{x} \in
\mathbb{R}^{N\times 1}$, making them both a column vector of size $N$, which
means that they can indeed be added together. 

We can also think about these optimization steps as continuously moving towards
some locally observed lowest point in the error landscape. The gradient
$\grad{L}$ is giving us the direction of steepest \textit{ascent}, which means
that the opposite direction, $-\grad{L}$ will be the direction of the steepest
\textit{descent} locally, guiding us towards some (potentially only local) error
minimum.

\todo{I will also refer back to these equations. Is it too redundant here to
mention that we will be using these steps for our results? I would also like to
highlight that this actually makes sense to write about from the perspective of
our thesis.}

\todo{Mentioning other optimization methods, such as Newton's method, and making
a connection with them via how they all approximate the Hessian matrix
differently, would have us introduce many other topics. For the same reason,
I don't plan on expanding upon Adam. Is this OK?}

\subsection*{Neural Networks}
The goal of \acf{DL} is to approximate an unknown function 

$$\vb{f}^*(\vb{x}) = \vb{y}^*,$$

where $\vb{y}^*$ denotes \textit{ground truth} solutions. $\vb{f}^*(\vb{x})$ is
approximated by a \acf{NN} representation 

$$\vb{f}(\vb{x}, \boldsymbol\theta) = \vb{y},$$

where $\boldsymbol{\theta}$ is a vector of \textit{weights}, influencing the output of
the \ac{NN}. \ac{DL} is about finding $\boldsymbol{\theta}$ parameters such
that the outputs of the \ac{NN} match the $\vb{y}^*$ outputs of the original
function $\vb{f}^*$ as closely as possible, as measured by some scalar-valued
loss function $L$:

$$\arg\min_{\boldsymbol\theta} L\big(\vb{f}(\vb{x}, \boldsymbol\theta), \vb{y}^*
    \big).$$

In the simplest case, using a mean-square error (also known as $L_2$
norm):

\begin{equation}\label{eq:NN-min}
    \arg\min_{\boldsymbol\theta} 
        L_2\big(\vb{f}(\vb{x}, \boldsymbol\theta), \vb{y}^* \big) 
    = \arg\min_{\boldsymbol\theta} 
        \lvert \vb{f}(\vb{x}, \boldsymbol\theta) - \vb{y}^* \rvert_2^2.
\end{equation}

As discussed in section~\ref{section:optimization}, we can use the
gradients of the loss function $L$ with respect to the weights
$\boldsymbol{\theta}$ (i.e. $\pdv*{L}{\boldsymbol\theta}$) to solve
equation~\eqref{eq:NN-min}, yielding the optimal $\boldsymbol{\theta}$
parameters. We optimize, i.e. \textit{train} our \ac{NN} with a \acf{SGD}
optimizer, such as Adam (\cite{adam}).

In the case of a fully-connected \ac{NN}, we can write its $i^{th}$ layer as 

\begin{equation}
    \vb{o}^i = \sigma\left(\vb{W}_i \vb{o}^{i-1} + b_i\right),
\end{equation}

where $\vb{o}^i$ is the output of the $i^{th}$ layer, $\sigma$ is a non-linear
activation function, such as the rectified linear unit (ReLU) function, and
$\vb{W}_i$ and $b_i$ are the weight matrix and the bias of layer $i$,
respectively. We call $\vb{W}_i$ and $b_i$ the parameters of the \ac{NN},
and collect their values from all layers in $\boldsymbol\theta$.

\todo{Should we write about backpropagation here, or that's only an
implementation practicality to be mentioned only later on?}

\todo{Would it look better to not write everything in bold (losing the notion of
all of these being vector-valued), i.e.
$$\arg\min_{\theta} L_2\big(f(x, \theta), y^*\big)\text{?}$$
}

